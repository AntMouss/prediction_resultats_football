{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données GOAT_all_fin est issu d'un ensemble de transformation que l'on a réalisé sur le jeu d'origine se trouvant sur le site kaggle:\n",
    "https://www.kaggle.com/hugomathien/soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13801, 52)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mon_enregistrement=pd.read_csv('GOAT_all_fin.csv')\n",
    "mon_enregistrement.tail()\n",
    "mon_enregistrement.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "except ImportError:\n",
    "    from future_encoders import OneHotEncoder\n",
    "from sklearn import preprocessing    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Création d'un jeu d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13801, 51)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches=mon_enregistrement\n",
    "X_df=matches.drop(['label'],axis=1)\n",
    "Y_df=matches['label']\n",
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 4 4 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeLimier\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13801, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on fait un one-hot-encoder pour la variable correspondant au championnat.\n",
    "matches_cat=X_df['league_name']\n",
    "enc = preprocessing.LabelEncoder()\n",
    "enc.fit(matches_cat)\n",
    "new_cat_features = enc.transform(matches_cat)\n",
    "print (new_cat_features) # [1 2 0]\n",
    "new_cat_features = new_cat_features.reshape(-1, 1) # On redimensionne\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False) #Plus facile à lire\n",
    "data_2=ohe.fit_transform(new_cat_features)\n",
    "data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13801, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on convertit le tableau numpy en DataFrame.\n",
    "data_2=pd.DataFrame(data_2)\n",
    "data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on enlève toutes les colonnes qui ne sont pas utiles pour l'entrainement et la prédiction.\n",
    "X_df=X_df.drop(['home_team','away_team','stage','country_name','league_name','season','date','id','home_team_goal','away_team_goal'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13801, 46)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df=pd.concat([data_2,X_df],axis=1)\n",
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On constitue un jeu de test et d'entrainement.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, Y_df, test_size = 0.2, random_state = 42,stratify = Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_paris=['B365H',\n",
    "'B365D',\n",
    "'B365A',\n",
    "'BWH',\n",
    "'BWD',\n",
    "'BWA',\n",
    "'IWH',\n",
    "'IWD',\n",
    "'IWA',\n",
    "'LBH',\n",
    "'LBD',\n",
    "'LBA',\n",
    "'PSH',\n",
    "'PSD',\n",
    "'PSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On constitue le DataFrame des paris qu'on utilisera une fois avoir trouvé notre modèle.\n",
    "les_paris=X_test[col_paris]\n",
    "X_train=X_train.drop(col_paris,axis=1)#on enlève les cotes des paris pour l'entrainement.\n",
    "X_test=X_test.drop(col_paris,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>home_player_1</th>\n",
       "      <th>home_player_2</th>\n",
       "      <th>home_player_3</th>\n",
       "      <th>home_player_4</th>\n",
       "      <th>home_player_5</th>\n",
       "      <th>...</th>\n",
       "      <th>away_player_6</th>\n",
       "      <th>away_player_7</th>\n",
       "      <th>away_player_8</th>\n",
       "      <th>away_player_9</th>\n",
       "      <th>away_player_10</th>\n",
       "      <th>away_player_11</th>\n",
       "      <th>vict_cons_dom</th>\n",
       "      <th>inv_dom</th>\n",
       "      <th>vict_cons_ext</th>\n",
       "      <th>inv_ext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>-7</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4  home_player_1  home_player_2  home_player_3  \\\n",
       "1766   0.0  1.0  0.0  0.0  0.0           75.0           65.0           71.0   \n",
       "2251   1.0  0.0  0.0  0.0  0.0           77.0           75.0           77.0   \n",
       "11936  0.0  0.0  1.0  0.0  0.0           80.0           72.0           74.0   \n",
       "9319   0.0  0.0  0.0  0.0  1.0           81.0           78.0           71.0   \n",
       "781    0.0  0.0  0.0  0.0  1.0           81.0           68.0           74.0   \n",
       "\n",
       "       home_player_4  home_player_5  ...  away_player_6  away_player_7  \\\n",
       "1766            70.0           71.0  ...           76.0           58.0   \n",
       "2251            78.0           75.0  ...           73.0           70.0   \n",
       "11936           77.0           74.0  ...           79.0           77.0   \n",
       "9319            80.0           72.0  ...           70.0           70.0   \n",
       "781             71.0           67.0  ...           73.0           77.0   \n",
       "\n",
       "       away_player_8  away_player_9  away_player_10  away_player_11  \\\n",
       "1766            75.0           75.0            73.0            74.0   \n",
       "2251            72.0           74.0            70.0            74.0   \n",
       "11936           76.0           81.0            76.0            78.0   \n",
       "9319            67.0           80.0            80.0            75.0   \n",
       "781             66.0           76.0            72.0            78.0   \n",
       "\n",
       "       vict_cons_dom  inv_dom  vict_cons_ext  inv_ext  \n",
       "1766               1        1             -1        1  \n",
       "2251               4        5             -1       -1  \n",
       "11936             -2        9              1        1  \n",
       "9319               3        5             -7       -5  \n",
       "781               -2        1             -2       -2  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rand_clf = RandomForestClassifier(n_estimators = 100, random_state = 1, class_weight = 'balanced')\n",
    "Ada_clf = AdaBoostClassifier(n_estimators = 100, random_state = 2)\n",
    "KNN_clf =  KNeighborsClassifier()\n",
    "GNB_clf = GaussianNB()\n",
    "\n",
    "\n",
    "clfs = [Rand_clf, Ada_clf,  KNN_clf, GNB_clf]\n",
    "\n",
    "#On établit une grille de paramètres pour nos classifieurs afin de trouver le modèle le plus\n",
    "# optimal avec une recherche en grille.\n",
    "feature_len = X_train.shape[1]\n",
    "scorer = make_scorer(accuracy_score)\n",
    "params_Rand = {'clf__max_features': ['auto', 'log2'], \n",
    "                 'dm_reduce__n_components': np.arange(8, feature_len, np.around(feature_len/4))}\n",
    "params_Ada = {'clf__learning_rate': np.linspace(0.5, 2, 5), \n",
    "                 'dm_reduce__n_components': np.arange(8, feature_len, np.around(feature_len/4))}\n",
    "params_KNN = {'clf__n_neighbors': [3, 5, 10], \n",
    "                  'dm_reduce__n_components': np.arange(8, feature_len, np.around(feature_len/4))}\n",
    "params_GNB = {'dm_reduce__n_components': np.arange(8, feature_len, np.around(feature_len/4))}\n",
    "\n",
    "\n",
    "params = {clfs[0]: params_Rand,\n",
    "              clfs[1]: params_Ada,\n",
    "              clfs[2]: params_KNN,\n",
    "              clfs[3]: params_GNB,\n",
    "              }\n",
    "\n",
    "#On initialise notre pca qui sera notre paramètre de réduction de dimensionnalité.\n",
    "pca = PCA()\n",
    "dm_reductions = [pca]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on utilise tous les coeurs de notre ordinateur.\n",
    "n_jobs = -1\n",
    "cv=5 #nombre de bloc pour la validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf, dm_reduction, X_train, y_train, cv, params, scorer, jobs, \n",
    "                     best_components = None, best_params = None):\n",
    "    \n",
    "    \n",
    "    #on chronomètre\n",
    "    start = time()\n",
    "    \n",
    "    #On définit un pipeline de transformation en explicitant le classifieurs et le type de réduction de dimensions.\n",
    "    ele = [('dm_reduce', dm_reduction), ('clf', clf)]\n",
    "    pipeline = Pipeline(ele)\n",
    "        \n",
    "    params['dm_reduce__n_components'] = params['dm_reduce__n_components'].astype(int)\n",
    "    #On trouve le meilleur classifieur avec une recherche en grille.\n",
    "    grid_obj = model_selection.GridSearchCV(pipeline, param_grid = params, scoring = scorer, cv = cv, n_jobs = jobs)\n",
    "    grid_obj.fit(X_train, y_train)\n",
    "    best_pipe = grid_obj.best_estimator_\n",
    "    \n",
    "        \n",
    "    end = time()\n",
    "    \n",
    "    #affiche les résultats\n",
    "    print(\"Entrainé {} en {:.1f} minutes\".format(clf.__class__.__name__, (end - start)/60))\n",
    "    \n",
    "    #On retourne le meilleurs classifieurs par type de modèle.\n",
    "    return best_pipe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(clf, dm_reduction, X_train, y_train, X_test, y_test, cv, params, scorer, jobs, **kwargs):\n",
    "    \n",
    "    \n",
    "    #on donne le type de classifieur et la méthode de réduction de dimensions.\n",
    "    print(\"On entraine un {} avec {}...\".format(clf.__class__.__name__, dm_reduction.__class__.__name__))\n",
    "    \n",
    "    #On entraine le classifieur\n",
    "    best_pipe = train_classifier(clf, dm_reduction, X_train, y_train, cv, params, scorer, jobs)\n",
    "    \n",
    "    clf.fit(best_pipe.named_steps['dm_reduce'].transform(X_train), y_train)\n",
    "    \n",
    "    \n",
    "    # On affiche les résultats\n",
    "    print(\"Score du {} pour le jeu d'entrainement: {:.4f}.\".format(clf.__class__.__name__, predict_labels(clf, best_pipe, X_train, y_train)))\n",
    "    print(\"Score du {} pour le jeu de test: {:.4f}.\".format(clf.__class__.__name__, predict_labels(clf, best_pipe, X_test, y_test)))\n",
    "    \n",
    "    #Return classifier, dm reduction, and label predictions for train and test set\n",
    "    return clf, best_pipe.named_steps['dm_reduce'], predict_labels(clf, best_pipe, X_train, y_train), predict_labels(clf, best_pipe, X_test, y_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(clf, best_pipe, X, y):\n",
    "    y_pred = clf.predict(best_pipe.named_steps['dm_reduce'].transform(X))\n",
    "    #Retourne le score de précision\n",
    "    return accuracy_score(y.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_classifier(classifiers, dm_reductions, scorer, X_t, y_t, X_v, y_v, cv, params, jobs):\n",
    "    '''on fait le tour de tout les classifieurs et les types de réduction de dimensions '''\n",
    "    \n",
    "    #on initialise les tableaux dans lesquels on met nos résultats \n",
    "    clfs_return = []\n",
    "    dm_reduce_return = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    #on boucle sur les types de réduction de dimensions\n",
    "    for dm in dm_reductions:\n",
    "        \n",
    "        #on boucle sur les types de classifieurs\n",
    "        for clf in clfs:\n",
    "            \n",
    "            #On fait la recherche en grille\n",
    "            clf, dm_reduce, train_score, test_score = train_predict(clf = clf, dm_reduction = dm, X_train = X_t, y_train = y_t,\n",
    "                                                      X_test = X_v, y_test = y_v, cv = cv,\n",
    "                                                      params = params[clf], scorer = scorer, jobs = jobs)\n",
    "            \n",
    "            #on complète les tableaux des résultats            \n",
    "            clfs_return.append(clf)\n",
    "            dm_reduce_return.append(dm_reduce)\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "    \n",
    "    #On retourne les résultats\n",
    "    return clfs_return, dm_reduce_return, train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On entraine un RandomForestClassifier avec PCA...\n",
      "Entrainé RandomForestClassifier en 1.1 minutes\n",
      "Score du RandomForestClassifier pour le jeu d'entrainement: 1.0000.\n",
      "Score du RandomForestClassifier pour le jeu de test: 0.5187.\n",
      "On entraine un AdaBoostClassifier avec PCA...\n",
      "Entrainé AdaBoostClassifier en 1.9 minutes\n",
      "Score du AdaBoostClassifier pour le jeu d'entrainement: 0.5361.\n",
      "Score du AdaBoostClassifier pour le jeu de test: 0.5270.\n",
      "On entraine un KNeighborsClassifier avec PCA...\n",
      "Entrainé KNeighborsClassifier en 0.3 minutes\n",
      "Score du KNeighborsClassifier pour le jeu d'entrainement: 0.6254.\n",
      "Score du KNeighborsClassifier pour le jeu de test: 0.4440.\n",
      "On entraine un GaussianNB avec PCA...\n",
      "Entrainé GaussianNB en 0.0 minutes\n",
      "Score du GaussianNB pour le jeu d'entrainement: 0.5257.\n",
      "Score du GaussianNB pour le jeu de test: 0.5255.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#On entraine tous les classifieurs et on les compare.\n",
    "clfs, dm_reductions, train_scores, test_scores = find_best_classifier(clfs, dm_reductions, scorer, X_train, y_train, \n",
    "                                                                     X_test, y_test, cv, \n",
    "                                                                      params, n_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                         criterion='gini', max_depth=None, max_features='auto',\n",
       "                         max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                         min_impurity_split=None, min_samples_leaf=1,\n",
       "                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                         n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                         random_state=1, verbose=0, warm_start=False),\n",
       "  AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                     n_estimators=100, random_state=2),\n",
       "  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                       weights='uniform'),\n",
       "  GaussianNB(priors=None, var_smoothing=1e-09)],\n",
       " [PCA(copy=True, iterated_power='auto', n_components=24, random_state=None,\n",
       "      svd_solver='auto', tol=0.0, whiten=False),\n",
       "  PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
       "      svd_solver='auto', tol=0.0, whiten=False),\n",
       "  PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
       "      svd_solver='auto', tol=0.0, whiten=False),\n",
       "  PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
       "      svd_solver='auto', tol=0.0, whiten=False)],\n",
       " [1.0, 0.5360507246376811, 0.6253623188405797, 0.5257246376811594],\n",
       " [0.5186526620789569,\n",
       "  0.5269829771821803,\n",
       "  0.4440420137631293,\n",
       "  0.5255342267294458])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs, dm_reductions, train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clfs[np.argmax(test_scores)].predict_proba(dm_reductions[np.argmax(test_scores)].transform(X_test))\n",
    "# on calcule la probabilité prédite pour chaque classe en utilisant notre meilleur classifieur (Adaboost) avec la meilleure methode\n",
    "# de réduction de dimensions(ACP à 8 axes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2761, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape #on vérifie les dimensions (il y a 3 colonnes correspondant aux probabilités pour les 3 classes defeat, draw, win dans cet ordre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Calcul des gains en fonction des cotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2761,), (2761, 15))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np=y_test.values #on convertit le Dataframe en tableau numpy\n",
    "y_test_np.shape , les_paris.shape # on vérifie les tailles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "les_paris.fillna(0) #on remplace les cotes qu'on ne connait pas par 0.(valeur manquante)\n",
    "les_paris=les_paris.values #on convertit en tableau numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notre algorithme fait -429.59000000000003 euros avec tout les organismes en pariant 1euro sur chaque paris à espérance de gain positive \n",
      "avec un nombre de paris gagnants nb1=58 et un nombre de paris perdants nb2=1075\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "nb1=0\n",
    "nb2=0\n",
    "for j in range (len(y_test_np)):\n",
    "\n",
    "    for k in range (5):        #on boucle sur le nombre d'organismes de paris=5\n",
    "        for i in range(3):\n",
    "\n",
    "            h= ((les_paris[j][i+3*k]-1)*y_pred[j][2-i])-(1-y_pred[j][2-i]) # on calcul l'éspérance de gain h si elle est supérieure à 2 , on simule un pari fictif\n",
    "            if (h>2) and ((y_test_np[j]=='win' and i==0) or (y_test_np[j]=='draw' and i==1) or (y_test_np[j]=='defeat' and i==2))  :\n",
    "                sum=sum+(les_paris[j][i+3*k]-1)\n",
    "                nb1=nb1+1\n",
    "            elif (h>2) and ((y_test_np[j]=='win' and i!=0) or (y_test_np[j]=='draw' and i!=1) or (y_test_np[j]=='defeat' and i!=2)):    \n",
    "                sum=sum-1\n",
    "                nb2=nb2+1\n",
    "print('notre algorithme fait '+str(sum)+' euros avec tout les organismes en pariant 1euro sur chaque paris à espérance de gain positive \\navec un nombre de paris gagnants nb1='+str(nb1)+' et un nombre de paris perdants nb2='+str(nb2))           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre prédicteur ne nous permet pas d'engendrer des gains positifs, on en déduit que le football est un sport trop incertain et les cotes sont trop bien ajustées par les bookmakers pour que l'on puisse tomber sur des paris réellement intéressants.\n",
    "On peut aussi considérer que nos données ne synthétisent pas suffisamment l'information qui caractérise le futur résultat d'un match , il nous faudrait des données tenant compte de la mise en place tactique des deux équipes ainsi que leur historique sur une période beaucoup plus longue , on a aussi un manque de données global car il y a peu de confrontations entre deux même équipes dans une saison et donc pas la possibilité de repérer des pattern dans la succession des résultats entre deux même équipes.\n",
    "De plus les algorithmes de machine Learning s'avèrent assez peu complexes mais je doute que l'on obtienne de meilleurs résultats avec des réseaux de neurones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
